# Class - 2022/09/29

[TOC]

##  Conditional Probability

### Independence

> Recall: Two events $A, B$ are <u>mutually exclusive</u> if $A\cap B = \O\rightarrow P(AB)=0$ (They cannot occur together).

Two events $A,B$ are <u>independent</u> if $P(A|B) = P(A)$ and $P(B|A)=P(B)$

That is, they have no effect on each o ther

The formula can also be written as:
$$
P(A|B) = P(A) = \frac{P(AB)}{P(B)} \\
P(A)P(B) = P(AB)
$$

### Proposition

* Assume $A\&B$ are independent, show $A\&B^c$ are also independent .

#### Proof

* $A= AB \cup AB^c$   (They are <u>Disjoint  = Mutually Exclusive</u>)
* $P(A) = P(AB\cup AB^c) = P(AB)+P(AB^c)$
* $P(A) = P(A)P(B)+P(AB^c)$
* $ P(AB^c) = P(A)-P(A)P(B) = P(A)(1-P(B))=P(A)P(B^c)$
* $P(A)P(B^c) = P(AB^c)$
* Therefore,  $A\&B^c$ are also independent. 

### Example

Suppose that chance Lannie earns an A in the course is $40\%$ and the chance that Lannie gets an A in Analysis is $70\%$. If these events are independent, find the probability that Lannie:

1. Gets A in both => $P(AP) = 0.28$
2. Gets A in neither => $P(A'P') = 0.18$
3. Gets exactly one A => $P(AP')+P(A'P) = 0.42+0.12 = 0.54$

|       | P                       | P'                        | Total |
| ----- | ----------------------- | ------------------------- | ----- |
| A     | P(AP) = 0.7*0.4 = 0.28  | P(AP') = 0.7*0.6 = 0.42   | 0.7   |
| A'    | P(A'P) = 0.3*0.4 = 0.12 | P(A'P') = 0.3*0.6  = 0.18 | 0.3   |
| Total | 0.4                     | 0.6                       | 1     |



Two cards are dealt sequentially without replacement. 

Suppose $A$ is the event the first card is SPADE. 

Suppose $B$ is the event the second card is SPADE. 

Find $P(B|A)$ and $P(A|B)$

Are these events independent?

![image-20221005140214807](C:\Users\addas\Desktop\Fall 2022 Class Notes\Probability Notes\images\20220929\image-20221005140214807.png)

$P(B|A) = \frac{12}{51}\ne \frac 14 = P(B)$ 

$P(B) = \frac{13}{52}\frac{12}{51}+\frac{39}{52}\frac{13}{51} = \frac 14$

$A \& B $ are **NOT** <u>independent</u>!

## Chapter 4: Random Variables

Recall: Experiment -----> Sample Space --------->*(new)* Real number

* Ex: flip 2 coins -----> S= {HH,HT,TH,TT} ------> See example below

### Definition

A <u>random variable</u> is a **function** that assigns a real number to each element in the sample space.

* We often use capital letters $X,Y,Z$ to represent $R.V.$
* $X: S\rightarrow \R$, $X(s)=r$
* Often express $X$ as its possible outcomes

#### Example:

* Flip 2 coins.
* Let $X$ be the $R.V.$ that counts the number of heads.
    * $X = \{0,1,2\}$
    * $X: S\rightarrow \R$
    * $X(HH) = 2$
    * $X(HT) = 1$
    * $X(TH) = 1$
    * $X(TT) = 0$
* Let $Y$ be the $RV$ that subtracts number of heads from number of tails
    * $Y = \{2,0,-2\}$
* Let $Z$ be the $RV$ that multiples number of heads by 3 and adds to number of tails squared
    * $Z = \{6,4\}$
    * $Z(HH) = 6$
    * $Z(HT) = 4$
    * $Z(TH) = 4$
    * $Z(TT) = 4$ 

* A <u>moral</u> is a number $RV$ defined on a given sample space, and it's not unique

* **However**: Once a $RV$ is defined, then <u>the probability distribution function</u><u>(PDF)</u> is unique

    * Examples for $f_X , f_Y, f_Z$

    * |       | $X$  | $f_X$ | $Y$  | $f_Y$ | $Z$  | $f_Z$ |
        | ----- | ---- | ----- | ---- | ----- | ---- | ----- |
        |       | 0    | 1/4   | 2    | 1/4   | 6    | 1/4   |
        |       | 1    | 1/2   | 0    | 1/2   | 4    | 3/4   |
        |       | 2    | 1/3   | -2   | 1/4   |      |       |
        | Total |      | 1     |      | 1     |      | 1     |

        

### Discrete Random Variables (DRV)

A <u>**Discrete Random Variable**</u> on a Sample Space $S$, is a Random Variable that takes on a <u>finite</u> number of values, or <u>countably infinite</u> number of values.

> * Countable:
>     * finite
>     * countably infinite set: $\N, \Z, 2\Z, \Q,......$ (all with equal sizes)
>     * think about it like gaps
> * Uncountable:
>     * $\R, (0,1), ......$ or worse
>     * no gaps 

Therefore, we have

 Experiment ------> $S$ --------> (Define) $X$ --------> $f_X $ 

#### Example:

* Flip a coin until a heads appears. $S = \{H, TH, TTH, TTTH, .....\}$
* Define $X$ be the $RV$ that counts the number of trials need to to get the first head.
    * $X=\{1,2,3,......\}$
    * $X$ is DRV

### Probability Mass Function (PMF)

Our **probability distribution function** for a **DRV** is called a <u>probability mass function</u> (pmf)

* EX: The pmf for a DRV, $X$

* $p:X\rightarrow [0,1]$

    * $0\le p(x)\le 1$ for all $x\in X$
    * $\displaystyle \sum_{x\in X}p(X) = 1$
    * $p(a) = P(X=a)$ (assigns probabilities to each x in X)

    > For the continuous mass function, the properties would change slightly
    >
    > * same
    > * change the sum to integral
    > * not $P(X=a)$

#### Example 

* A pmf for a DRV, $I$, is given by:

    * $p:I\rightarrow [0,1]$,   $\displaystyle p(i) = \frac{c\lambda^i}{i!}$,   $i = 0,1,2,...(I = \{0,1,2,...\})$

* Find the constant $c$.

$$
\sum_{i\in I}p(i) = 1 = \sum_{i=0}^\infty \frac{c\lambda^i}{i!} = c\sum_{i=0}^\infty \frac{\lambda^i}{i!}
$$

> ==Recall==: $\displaystyle e^x = \sum^\infty_{i=0}\frac{x^i}{i!}$

$$
\sum_{i=0}^\infty \frac{c\lambda^i}{i!} = c\sum_{i=0}^\infty \frac{\lambda^i}{i!} = c\cdot e^\lambda = 1\\
\quad c = \frac 1{e^\lambda} = e^{-\lambda}
$$

* pmf: $\displaystyle p(i) = \frac{e^{-\lambda}\lambda^i}{i!}$

    > PMF for an important named DRV

* $\displaystyle p(0) = \frac{e^{-\lambda}\lambda^0}{0!} = e^{-\lambda}$

* $\displaystyle P(I\ge 2) = 1-P(I\le 1)= 1-p(0)-p(1)= 1-e^{-\lambda} - e^{-\lambda}\lambda$

    
